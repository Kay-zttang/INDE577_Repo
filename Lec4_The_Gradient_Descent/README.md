## The Gradient Descent
- **Field**: Unconstrained continuous optimization
- **First-order Method**

---
### **Concept**
The Concept of Gradient Descent is that "Gradient" (derivative for single variable function) provides a direction to "Descent" (optimize the function). So there're two important features to consider:

1. Descent direction

2. Learning Rate $\alpha$

$$
w_{n+1} = w_n - \alpha f'(w_n)  (\text{Gradient Descent Update Rule for a Function of one Variable})
$$

```math
w_{n+1} = w_n - \alpha f'(w_n) 
```

Always to find a suitable $\alpha$ through experimentaton.



---

### **Implementation**

#### **Dataset Description**

#### **Performance Conclusion**