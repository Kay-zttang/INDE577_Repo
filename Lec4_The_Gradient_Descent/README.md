## The Gradient Descent
- **Field**: Unconstrained continuous optimization
- **First-order Method**

---
### **Concept**
The Concept of Gradient Descent is that "Gradient" (derivative for single variable function) provides a direction to "Descent" (optimize the function). So there're two important features to consider:

1. **Descent direction**

2. **Learning Rate $\alpha$**

Update Rule for $f$ of one Variable
```math
Update\:Rule\:for\:f\:of\:one\:Variable:\; w_{n+1} = w_n - \alpha f'(w_n) 
```

Always to find a suitable $\alpha$ through experimentaton.



---

### **Implementation**

#### **Dataset Description**

#### **Performance Conclusion**